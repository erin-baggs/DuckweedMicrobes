{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1ySP1rBDygg3mRHkOkzV9O2jW-cNc-ftm",
      "authorship_tag": "ABX9TyPbB9qsAwkZrsYKi7n5PADM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/erin-baggs/DuckweedMicrobes/blob/main/ITS_Data_Processing_UNITE_update.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Google co-lab notebook for ITS rRNA amplicon sequencing visualization "
      ],
      "metadata": {
        "id": "o6peuUAsLyoP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Click on left panel, navigate to folder icon on the far left. Then at the top of the file bar click on the folder with the google drive symbol and agree to mount google drive. "
      ],
      "metadata": {
        "id": "6ICOX6u_GXjL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://colab.research.google.com/drive/1BK2-d7xcF1puNO4h1lFgRMhgnKAuTKzh#scrollTo=I9a7MtewFIAH\n"
      ],
      "metadata": {
        "id": "4OHFbDphybmg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9a7MtewFIAH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "733b27ae-bec3-4f02-f55e-1066d0e12587"
      },
      "source": [
        "# The Google Colab Environment does not have conda set, this would\n",
        "# ordinarily be the easies option to install these tools.\n",
        "\n",
        "!pip install git+https://github.com/rrwick/Porechop.git  # just so pomoxis will install cleanly\n",
        "!pip install medaka pomoxis aplanat intervaltree==3.0.2\n",
        "# install samtools from source\n",
        "!wget https://github.com/samtools/samtools/releases/download/1.10/samtools-1.10.tar.bz2\n",
        "!tar -xjf samtools-1.10.tar.bz2\n",
        "!cd samtools-1.10 && ./configure --prefix=/usr/local/ && make && make install\n",
        "!wget https://github.com/lh3/minimap2/releases/download/v2.17/minimap2-2.17_x64-linux.tar.bz2\n",
        "!tar -xjf minimap2-2.17_x64-linux.tar.bz2\n",
        "!cp /content/minimap2-2.17_x64-linux/minimap2 /usr/local/bin/\n",
        "!pip install requests "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/rrwick/Porechop.git\n",
            "  Cloning https://github.com/rrwick/Porechop.git to /tmp/pip-req-build-46tbsnmk\n",
            "  Running command git clone -q https://github.com/rrwick/Porechop.git /tmp/pip-req-build-46tbsnmk\n",
            "Building wheels for collected packages: porechop\n",
            "  Building wheel for porechop (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for porechop: filename=porechop-0.2.4-py3-none-any.whl size=81935 sha256=c44d59fe3ea94389578395f9d393cbe78cee6b304ad2951febda31781030e86d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-2dut_3vc/wheels/42/d0/87/2d0ecb7eabe721fb0fe9bf5879c5e6a2a95b76b8e12a4eaa59\n",
            "Successfully built porechop\n",
            "Installing collected packages: porechop\n",
            "Successfully installed porechop-0.2.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting medaka\n",
            "  Downloading medaka-1.7.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (41.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 41.0 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting pomoxis\n",
            "  Downloading pomoxis-0.3.10.tar.gz (95 kB)\n",
            "\u001b[K     |████████████████████████████████| 95 kB 4.4 MB/s \n",
            "\u001b[?25hCollecting aplanat\n",
            "  Downloading aplanat-0.6.11.tar.gz (298 kB)\n",
            "\u001b[K     |████████████████████████████████| 298 kB 51.7 MB/s \n",
            "\u001b[?25hCollecting intervaltree==3.0.2\n",
            "  Downloading intervaltree-3.0.2.tar.gz (30 kB)\n",
            "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from intervaltree==3.0.2) (2.4.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from medaka) (3.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from medaka) (2.23.0)\n",
            "Collecting pysam>=0.16.0.1\n",
            "  Downloading pysam-0.20.0-cp37-cp37m-manylinux_2_24_x86_64.whl (15.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.4 MB 4.3 MB/s \n",
            "\u001b[?25hCollecting ont-fast5-api\n",
            "  Downloading ont_fast5_api-4.1.0-py3-none-any.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 40.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from medaka) (1.21.6)\n",
            "Requirement already satisfied: grpcio in /usr/local/lib/python3.7/dist-packages (from medaka) (1.50.0)\n",
            "Collecting mappy\n",
            "  Downloading mappy-2.24.tar.gz (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 68.2 MB/s \n",
            "\u001b[?25hCollecting parasail\n",
            "  Downloading parasail-1.3.3-py2.py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.6 MB 8.3 MB/s \n",
            "\u001b[?25hCollecting pyspoa>=0.0.3\n",
            "  Downloading pyspoa-0.0.8-cp37-cp37m-manylinux2010_x86_64.whl (878 kB)\n",
            "\u001b[K     |████████████████████████████████| 878 kB 39.6 MB/s \n",
            "\u001b[?25hCollecting cffi==1.15.0\n",
            "  Downloading cffi-1.15.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (427 kB)\n",
            "\u001b[K     |████████████████████████████████| 427 kB 61.3 MB/s \n",
            "\u001b[?25hCollecting edlib\n",
            "  Downloading edlib-1.3.9-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
            "\u001b[K     |████████████████████████████████| 325 kB 68.3 MB/s \n",
            "\u001b[?25hCollecting tensorflow~=2.7.0\n",
            "  Downloading tensorflow-2.7.4-cp37-cp37m-manylinux2010_x86_64.whl (495.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 495.5 MB 12 kB/s \n",
            "\u001b[?25hRequirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi==1.15.0->medaka) (2.21)\n",
            "Collecting pybind11>=2.4\n",
            "  Downloading pybind11-2.10.1-py3-none-any.whl (216 kB)\n",
            "\u001b[K     |████████████████████████████████| 216 kB 64.0 MB/s \n",
            "\u001b[?25hCollecting cmake==3.18.4\n",
            "  Downloading cmake-3.18.4-py3-none-manylinux1_x86_64.whl (17.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 17.7 MB 34.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->medaka) (2.0.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->medaka) (0.37.1)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->medaka) (0.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->medaka) (4.1.1)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->medaka) (1.3.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->medaka) (3.17.3)\n",
            "Collecting keras<2.8,>=2.7.0rc0\n",
            "  Downloading keras-2.7.0-py2.py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 55.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->medaka) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->medaka) (1.14.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->medaka) (1.1.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->medaka) (1.6.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->medaka) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->medaka) (0.27.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->medaka) (14.0.6)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->medaka) (2.9.1)\n",
            "Collecting tensorflow-estimator<2.8,~=2.7.0rc0\n",
            "  Downloading tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)\n",
            "\u001b[K     |████████████████████████████████| 463 kB 44.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->medaka) (1.12)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->medaka) (3.3.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->medaka) (1.5.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow~=2.7.0->medaka) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow~=2.7.0->medaka) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow~=2.7.0->medaka) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow~=2.7.0->medaka) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow~=2.7.0->medaka) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow~=2.7.0->medaka) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow~=2.7.0->medaka) (1.35.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow~=2.7.0->medaka) (4.9)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow~=2.7.0->medaka) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow~=2.7.0->medaka) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow~=2.7.0->medaka) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow~=2.7.0->medaka) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow~=2.7.0->medaka) (3.10.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow~=2.7.0->medaka) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->medaka) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->medaka) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->medaka) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->medaka) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow~=2.7.0->medaka) (3.2.2)\n",
            "Collecting biopython<1.77,>=1.63\n",
            "  Downloading biopython-1.76-cp37-cp37m-manylinux1_x86_64.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 42.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from pomoxis) (3.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from pomoxis) (1.3.5)\n",
            "Requirement already satisfied: Porechop in /usr/local/lib/python3.7/dist-packages (from pomoxis) (0.2.4)\n",
            "Requirement already satisfied: bokeh in /usr/local/lib/python3.7/dist-packages (from aplanat) (2.3.3)\n",
            "Collecting icon_font_to_png\n",
            "  Downloading icon_font_to_png-0.4.1-py2.py3-none-any.whl (161 kB)\n",
            "\u001b[K     |████████████████████████████████| 161 kB 56.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from aplanat) (7.1.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from aplanat) (1.7.3)\n",
            "Collecting si-prefix\n",
            "  Downloading si-prefix-1.2.2.tar.gz (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 3.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from aplanat) (1.0.2)\n",
            "Collecting sigfig\n",
            "  Downloading sigfig-1.3.2-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->pomoxis) (2022.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->pomoxis) (2.8.2)\n",
            "Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.7/dist-packages (from bokeh->aplanat) (2.11.3)\n",
            "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.7/dist-packages (from bokeh->aplanat) (6.0.4)\n",
            "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.7/dist-packages (from bokeh->aplanat) (21.3)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.7/dist-packages (from bokeh->aplanat) (6.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.9->bokeh->aplanat) (2.0.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=16.8->bokeh->aplanat) (3.0.9)\n",
            "Collecting tinycss>=0.4\n",
            "  Downloading tinycss-0.4.tar.gz (87 kB)\n",
            "\u001b[K     |████████████████████████████████| 87 kB 6.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pomoxis) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pomoxis) (1.4.4)\n",
            "Collecting progressbar33>=2.3.1\n",
            "  Downloading progressbar33-2.4.tar.gz (10 kB)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->aplanat) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->aplanat) (3.1.0)\n",
            "Building wheels for collected packages: intervaltree, pomoxis, aplanat, tinycss, mappy, progressbar33, si-prefix\n",
            "  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for intervaltree: filename=intervaltree-3.0.2-py3-none-any.whl size=25805 sha256=679fc64d5da9b8064c71d0a7392fe4dc61b03d3f0a233a4ede91f0c2883be0d8\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/2d/6f/68fdd1342f58dac32f7d2781b00e9fd43b15d51e4c1ff9b4fb\n",
            "  Building wheel for pomoxis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pomoxis: filename=pomoxis-0.3.10-py3-none-any.whl size=61447 sha256=75ec3e027370a5bd00c3923b3f7f89eb027c9c2a339653781cd8cca54184f868\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/d7/5c/8fe00c0c6c4a896b8befa8941f001da81b97aa04583d0ddc1a\n",
            "  Building wheel for aplanat (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for aplanat: filename=aplanat-0.6.11-py3-none-any.whl size=307264 sha256=00c944195881106bd4e8714668f2b1d74d383bfb190a0f357c4e1714e297453d\n",
            "  Stored in directory: /root/.cache/pip/wheels/13/fb/93/5a78e0b6c4d9d6e715e9a2712ec24834af1602bced73bb8db9\n",
            "  Building wheel for tinycss (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinycss: filename=tinycss-0.4-py3-none-any.whl size=43955 sha256=d31b74f1ca71690163cde956a38e62857ccde1d222a49ab08f3b27c35b62e7fb\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/66/e8/e53d7a476011891fa51a5ee83a2d1852b19b258f975055429b\n",
            "  Building wheel for mappy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mappy: filename=mappy-2.24-cp37-cp37m-linux_x86_64.whl size=507343 sha256=5a7663cd753575421fb0c654450ac60c12e4720932c05031c6a74c054089c45e\n",
            "  Stored in directory: /root/.cache/pip/wheels/8e/61/bb/06a5da2c026d171df0ae3a58dfe86da56187323652da1a0194\n",
            "  Building wheel for progressbar33 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for progressbar33: filename=progressbar33-2.4-py3-none-any.whl size=12156 sha256=559fd9e1941cff991ff87e3f6a70215d8bdc0674e67fd76654e0afd0782c1dc5\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/34/90/051529053958a4c8e6b33b7c2c2a7de679cfb62703da0383d2\n",
            "  Building wheel for si-prefix (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for si-prefix: filename=si_prefix-1.2.2-py3-none-any.whl size=5876 sha256=9bddcac81c30c391bf3d3ab8eb7718adcbb089917cf770a9308c9b6a2d89b2bf\n",
            "  Stored in directory: /root/.cache/pip/wheels/36/42/93/1b528e2666e5d7b99e12e76674b92fcc67610c7251e298dea0\n",
            "Successfully built intervaltree pomoxis aplanat tinycss mappy progressbar33 si-prefix\n",
            "Installing collected packages: tinycss, tensorflow-estimator, pybind11, progressbar33, keras, cmake, tensorflow, sigfig, si-prefix, pyspoa, pysam, parasail, ont-fast5-api, mappy, intervaltree, icon-font-to-png, edlib, cffi, biopython, pomoxis, medaka, aplanat\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.9.0\n",
            "    Uninstalling tensorflow-estimator-2.9.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.9.0\n",
            "    Uninstalling keras-2.9.0:\n",
            "      Successfully uninstalled keras-2.9.0\n",
            "  Attempting uninstall: cmake\n",
            "    Found existing installation: cmake 3.22.6\n",
            "    Uninstalling cmake-3.22.6:\n",
            "      Successfully uninstalled cmake-3.22.6\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.9.2\n",
            "    Uninstalling tensorflow-2.9.2:\n",
            "      Successfully uninstalled tensorflow-2.9.2\n",
            "  Attempting uninstall: intervaltree\n",
            "    Found existing installation: intervaltree 2.1.0\n",
            "    Uninstalling intervaltree-2.1.0:\n",
            "      Successfully uninstalled intervaltree-2.1.0\n",
            "  Attempting uninstall: cffi\n",
            "    Found existing installation: cffi 1.15.1\n",
            "    Uninstalling cffi-1.15.1:\n",
            "      Successfully uninstalled cffi-1.15.1\n",
            "Successfully installed aplanat-0.6.11 biopython-1.76 cffi-1.15.0 cmake-3.18.4 edlib-1.3.9 icon-font-to-png-0.4.1 intervaltree-3.0.2 keras-2.7.0 mappy-2.24 medaka-1.7.2 ont-fast5-api-4.1.0 parasail-1.3.3 pomoxis-0.3.10 progressbar33-2.4 pybind11-2.10.1 pysam-0.20.0 pyspoa-0.0.8 si-prefix-1.2.2 sigfig-1.3.2 tensorflow-2.7.4 tensorflow-estimator-2.7.0 tinycss-0.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cffi"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-04 21:33:24--  https://github.com/samtools/samtools/releases/download/1.10/samtools-1.10.tar.bz2\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/3666841/207d0e80-1848-11ea-9eb4-eeeea8d26cef?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20221104%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20221104T213324Z&X-Amz-Expires=300&X-Amz-Signature=496ae2f4bfc4320dcc28954f04949bdade66fb71761c2d3918b39d549c14f9c0&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=3666841&response-content-disposition=attachment%3B%20filename%3Dsamtools-1.10.tar.bz2&response-content-type=application%2Foctet-stream [following]\n",
            "--2022-11-04 21:33:24--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/3666841/207d0e80-1848-11ea-9eb4-eeeea8d26cef?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20221104%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20221104T213324Z&X-Amz-Expires=300&X-Amz-Signature=496ae2f4bfc4320dcc28954f04949bdade66fb71761c2d3918b39d549c14f9c0&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=3666841&response-content-disposition=attachment%3B%20filename%3Dsamtools-1.10.tar.bz2&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4721173 (4.5M) [application/octet-stream]\n",
            "Saving to: ‘samtools-1.10.tar.bz2’\n",
            "\n",
            "samtools-1.10.tar.b 100%[===================>]   4.50M  9.71MB/s    in 0.5s    \n",
            "\n",
            "2022-11-04 21:33:25 (9.71 MB/s) - ‘samtools-1.10.tar.bz2’ saved [4721173/4721173]\n",
            "\n",
            "checking for gcc... gcc\n",
            "checking whether the C compiler works... yes\n",
            "checking for C compiler default output file name... a.out\n",
            "checking for suffix of executables... \n",
            "checking whether we are cross compiling... no\n",
            "checking for suffix of object files... o\n",
            "checking whether we are using the GNU C compiler... yes\n",
            "checking whether gcc accepts -g... yes\n",
            "checking for gcc option to accept ISO C89... none needed\n",
            "checking for grep that handles long lines and -e... /bin/grep\n",
            "checking for C compiler warning flags... -Wall\n",
            "checking for special C compiler options needed for large files... no\n",
            "checking for _FILE_OFFSET_BITS value needed for large files... no\n",
            "checking location of HTSlib source tree... htslib-1.10\n",
            "checking for NcursesW wide-character library... no\n",
            "checking for Ncurses library... yes\n",
            "checking for working ncurses/curses.h... no\n",
            "checking for working ncurses.h... yes\n",
            "checking for zlib.h... yes\n",
            "checking for inflate in -lz... yes\n",
            "checking for library containing regcomp... none required\n",
            "configure: creating ./config.status\n",
            "config.status: creating config.mk\n",
            "config.status: creating config.h\n",
            "=== configuring in htslib-1.10 (/content/samtools-1.10/htslib-1.10)\n",
            "configure: running /bin/bash ./configure --disable-option-checking '--prefix=/usr/local'  --cache-file=/dev/null --srcdir=.\n",
            "checking for gcc... gcc\n",
            "checking whether the C compiler works... yes\n",
            "checking for C compiler default output file name... a.out\n",
            "checking for suffix of executables... \n",
            "checking whether we are cross compiling... no\n",
            "checking for suffix of object files... o\n",
            "checking whether we are using the GNU C compiler... yes\n",
            "checking whether gcc accepts -g... yes\n",
            "checking for gcc option to accept ISO C89... none needed\n",
            "checking for ranlib... ranlib\n",
            "checking for grep that handles long lines and -e... /bin/grep\n",
            "checking for C compiler warning flags... -Wall\n",
            "checking for pkg-config... /usr/bin/pkg-config\n",
            "checking pkg-config is at least version 0.9.0... yes\n",
            "checking for special C compiler options needed for large files... no\n",
            "checking for _FILE_OFFSET_BITS value needed for large files... no\n",
            "checking shared library type for unknown-Linux... plain .so\n",
            "checking whether the compiler accepts -fvisibility=hidden... yes\n",
            "checking how to run the C preprocessor... gcc -E\n",
            "checking for egrep... /bin/grep -E\n",
            "checking for ANSI C header files... yes\n",
            "checking for sys/types.h... yes\n",
            "checking for sys/stat.h... yes\n",
            "checking for stdlib.h... yes\n",
            "checking for string.h... yes\n",
            "checking for memory.h... yes\n",
            "checking for strings.h... yes\n",
            "checking for inttypes.h... yes\n",
            "checking for stdint.h... yes\n",
            "checking for unistd.h... yes\n",
            "checking for stdlib.h... (cached) yes\n",
            "checking for unistd.h... (cached) yes\n",
            "checking for sys/param.h... yes\n",
            "checking for getpagesize... yes\n",
            "checking for working mmap... yes\n",
            "checking for gmtime_r... yes\n",
            "checking for fsync... yes\n",
            "checking for drand48... yes\n",
            "checking whether fdatasync is declared... yes\n",
            "checking for fdatasync... yes\n",
            "checking for library containing log... -lm\n",
            "checking for zlib.h... yes\n",
            "checking for inflate in -lz... yes\n",
            "checking for library containing recv... none required\n",
            "checking for bzlib.h... yes\n",
            "checking for BZ2_bzBuffToBuffCompress in -lbz2... yes\n",
            "checking for lzma.h... yes\n",
            "checking for lzma_easy_buffer_encode in -llzma... yes\n",
            "checking for libdeflate.h... no\n",
            "checking for libdeflate_deflate_compress in -ldeflate... no\n",
            "checking for curl_easy_pause in -lcurl... yes\n",
            "checking for CCHmac... no\n",
            "checking for library containing HMAC... -lcrypto\n",
            "checking whether PTHREAD_MUTEX_RECURSIVE is declared... yes\n",
            "configure: creating ./config.status\n",
            "config.status: creating config.mk\n",
            "config.status: creating htslib.pc.tmp\n",
            "config.status: creating config.h\n",
            "config.mk:45: htslib-1.10/htslib_static.mk: No such file or directory\n",
            "cd htslib-1.10 && make htslib_static.mk\n",
            "make[1]: Entering directory '/content/samtools-1.10/htslib-1.10'\n",
            "sed -n '/^static_libs=/s/[^=]*=/HTSLIB_static_LIBS = /p;/^static_ldflags=/s/[^=]*=/HTSLIB_static_LDFLAGS = /p' htslib.pc.tmp > htslib_static.mk\n",
            "make[1]: Leaving directory '/content/samtools-1.10/htslib-1.10'\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bam_index.o bam_index.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bam_plcmd.o bam_plcmd.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o sam_view.o sam_view.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bam_fastq.o bam_fastq.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bam_cat.o bam_cat.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bam_md.o bam_md.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bam_reheader.o bam_reheader.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bam_sort.o bam_sort.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bedidx.o bedidx.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bam_rmdup.o bam_rmdup.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bam_rmdupse.o bam_rmdupse.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bam_mate.o bam_mate.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bam_stat.o bam_stat.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bam_color.o bam_color.c\n",
            "echo '#define SAMTOOLS_VERSION \"1.10\"' > version.h\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bamtk.o bamtk.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bam2bcf.o bam2bcf.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bam2bcf_indel.o bam2bcf_indel.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o sample.o sample.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o cut_target.o cut_target.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o phase.o phase.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bam2depth.o bam2depth.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o coverage.o coverage.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o padding.o padding.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bedcov.o bedcov.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bamshuf.o bamshuf.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o faidx.o faidx.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o dict.o dict.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o stats.o stats.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o stats_isize.o stats_isize.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bam_flags.o bam_flags.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bam_split.o bam_split.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bam_tview.o bam_tview.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bam_tview_curses.o bam_tview_curses.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bam_tview_html.o bam_tview_html.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bam_lpileup.o bam_lpileup.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bam_quickcheck.o bam_quickcheck.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bam_addrprg.o bam_addrprg.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bam_markdup.o bam_markdup.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o tmp_file.o tmp_file.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o lz4/lz4.o lz4/lz4.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bam_aux.o bam_aux.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bam.o bam.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o sam.o sam.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bam_plbuf.o bam_plbuf.c\n",
            "ar -csru libbam.a bam_aux.o bam.o sam.o bam_plbuf.o\n",
            "ar: `u' modifier ignored since `D' is the default (see `U')\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o sam_opts.o sam_opts.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o sam_utils.o sam_utils.c\n",
            "ar -rcs libst.a sam_opts.o sam_utils.o\n",
            "cd htslib-1.10 && make hts-object-files\n",
            "make[1]: Entering directory '/content/samtools-1.10/htslib-1.10'\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o kfunc.o kfunc.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o knetfile.o knetfile.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o kstring.o kstring.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o bcf_sr_sort.o bcf_sr_sort.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o bgzf.o bgzf.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o errmod.o errmod.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o faidx.o faidx.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o header.o header.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o hfile.o hfile.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o hfile_net.o hfile_net.c\n",
            "echo '#define HTS_VERSION_TEXT \"1.10\"' > version.h\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o hts.o hts.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o hts_os.o hts_os.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o md5.o md5.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o multipart.o multipart.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o probaln.o probaln.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o realn.o realn.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o regidx.o regidx.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o region.o region.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o sam.o sam.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o synced_bcf_reader.o synced_bcf_reader.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o vcf_sweep.o vcf_sweep.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o tbx.o tbx.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o textutils.o textutils.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o thread_pool.o thread_pool.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o vcf.o vcf.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o vcfutils.o vcfutils.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o cram/cram_codecs.o cram/cram_codecs.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o cram/cram_decode.o cram/cram_decode.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o cram/cram_encode.o cram/cram_encode.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o cram/cram_external.o cram/cram_external.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o cram/cram_index.o cram/cram_index.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o cram/cram_io.o cram/cram_io.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o cram/cram_samtools.o cram/cram_samtools.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o cram/cram_stats.o cram/cram_stats.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o cram/mFILE.o cram/mFILE.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o cram/open_trace_file.o cram/open_trace_file.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o cram/pooled_alloc.o cram/pooled_alloc.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o cram/rANS_static.o cram/rANS_static.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o cram/string_alloc.o cram/string_alloc.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o hfile_libcurl.o hfile_libcurl.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o hfile_gcs.o hfile_gcs.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o hfile_s3.o hfile_s3.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o hfile_s3_write.o hfile_s3_write.c\n",
            "touch hts-object-files\n",
            "make[1]: Leaving directory '/content/samtools-1.10/htslib-1.10'\n",
            "cd htslib-1.10 && make lib-static\n",
            "make[1]: Entering directory '/content/samtools-1.10/htslib-1.10'\n",
            "ar -rc libhts.a kfunc.o knetfile.o kstring.o bcf_sr_sort.o bgzf.o errmod.o faidx.o header.o hfile.o hfile_net.o hts.o hts_os.o md5.o multipart.o probaln.o realn.o regidx.o region.o sam.o synced_bcf_reader.o vcf_sweep.o tbx.o textutils.o thread_pool.o vcf.o vcfutils.o cram/cram_codecs.o cram/cram_decode.o cram/cram_encode.o cram/cram_external.o cram/cram_index.o cram/cram_io.o cram/cram_samtools.o cram/cram_stats.o cram/mFILE.o cram/open_trace_file.o cram/pooled_alloc.o cram/rANS_static.o cram/string_alloc.o   hfile_libcurl.o hfile_gcs.o hfile_s3.o hfile_s3_write.o\n",
            "ranlib libhts.a\n",
            "make[1]: Leaving directory '/content/samtools-1.10/htslib-1.10'\n",
            "gcc  -L./lz4  -o samtools bam_index.o bam_plcmd.o sam_view.o bam_fastq.o bam_cat.o bam_md.o bam_reheader.o bam_sort.o bedidx.o bam_rmdup.o bam_rmdupse.o bam_mate.o bam_stat.o bam_color.o bamtk.o bam2bcf.o bam2bcf_indel.o sample.o cut_target.o phase.o bam2depth.o coverage.o padding.o bedcov.o bamshuf.o faidx.o dict.o stats.o stats_isize.o bam_flags.o bam_split.o bam_tview.o bam_tview_curses.o bam_tview_html.o bam_lpileup.o bam_quickcheck.o bam_addrprg.o bam_markdup.o tmp_file.o ./lz4/lz4.o libbam.a libst.a htslib-1.10/libhts.a -lpthread -lz -lm -lbz2 -llzma -lcurl -lcrypto -lncurses -lm -lz  -lpthread\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o misc/ace2sam.o misc/ace2sam.c\n",
            "gcc  -o misc/ace2sam misc/ace2sam.o -lz \n",
            "gcc -Wall -g -O2 -DMAQ_LONGREADS -I. -Ihtslib-1.10 -I./lz4  -c -o misc/maq2sam-long.o misc/maq2sam.c\n",
            "gcc  -o misc/maq2sam-long misc/maq2sam-long.o -lz \n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o misc/maq2sam-short.o misc/maq2sam.c\n",
            "gcc  -o misc/maq2sam-short misc/maq2sam-short.o -lz \n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o misc/md5fa.o misc/md5fa.c\n",
            "gcc  -L./lz4  -o misc/md5fa misc/md5fa.o htslib-1.10/libhts.a -lpthread -lz -lm -lbz2 -llzma -lcurl -lcrypto -lz \n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o misc/md5sum-lite.o misc/md5sum-lite.c\n",
            "gcc  -L./lz4  -o misc/md5sum-lite misc/md5sum-lite.o htslib-1.10/libhts.a -lpthread -lz -lm -lbz2 -llzma -lcurl -lcrypto -lz \n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o misc/wgsim.o misc/wgsim.c\n",
            "gcc  -L./lz4  -o misc/wgsim misc/wgsim.o -lm htslib-1.10/libhts.a -lpthread -lz -lm -lbz2 -llzma -lcurl -lcrypto -lz \n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o test/merge/test_bam_translate.o test/merge/test_bam_translate.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o test/test.o test/test.c\n",
            "gcc  -L./lz4  -o test/merge/test_bam_translate test/merge/test_bam_translate.o test/test.o libst.a htslib-1.10/libhts.a -lpthread -lz -lm -lbz2 -llzma -lcurl -lcrypto -lz  -lpthread\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o test/merge/test_rtrans_build.o test/merge/test_rtrans_build.c\n",
            "gcc  -L./lz4  -o test/merge/test_rtrans_build test/merge/test_rtrans_build.o test/test.o libst.a htslib-1.10/libhts.a -lpthread -lz -lm -lbz2 -llzma -lcurl -lcrypto -lz  -lpthread\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o test/merge/test_trans_tbl_init.o test/merge/test_trans_tbl_init.c\n",
            "gcc  -L./lz4  -o test/merge/test_trans_tbl_init test/merge/test_trans_tbl_init.o test/test.o libst.a htslib-1.10/libhts.a -lpthread -lz -lm -lbz2 -llzma -lcurl -lcrypto -lz  -lpthread\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o test/split/test_count_rg.o test/split/test_count_rg.c\n",
            "gcc  -L./lz4  -o test/split/test_count_rg test/split/test_count_rg.o test/test.o libst.a htslib-1.10/libhts.a -lpthread -lz -lm -lbz2 -llzma -lcurl -lcrypto -lz  -lpthread\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o test/split/test_expand_format_string.o test/split/test_expand_format_string.c\n",
            "gcc  -L./lz4  -o test/split/test_expand_format_string test/split/test_expand_format_string.o test/test.o libst.a htslib-1.10/libhts.a -lpthread -lz -lm -lbz2 -llzma -lcurl -lcrypto -lz  -lpthread\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o test/split/test_filter_header_rg.o test/split/test_filter_header_rg.c\n",
            "gcc  -L./lz4  -o test/split/test_filter_header_rg test/split/test_filter_header_rg.o test/test.o libst.a htslib-1.10/libhts.a -lpthread -lz -lm -lbz2 -llzma -lcurl -lcrypto -lz  -lpthread\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o test/split/test_parse_args.o test/split/test_parse_args.c\n",
            "gcc  -L./lz4  -o test/split/test_parse_args test/split/test_parse_args.o test/test.o libst.a htslib-1.10/libhts.a -lpthread -lz -lm -lbz2 -llzma -lcurl -lcrypto -lz  -lpthread\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o test/vcf-miniview.o test/vcf-miniview.c\n",
            "gcc  -L./lz4  -o test/vcf-miniview test/vcf-miniview.o htslib-1.10/libhts.a -lpthread -lz -lm -lbz2 -llzma -lcurl -lcrypto -lz  -lpthread\n",
            "mkdir -p -m 755 /usr/local/bin /usr/local/bin /usr/local/share/man/man1\n",
            "install -p samtools /usr/local/bin\n",
            "install -p misc/ace2sam misc/maq2sam-long misc/maq2sam-short misc/md5fa misc/md5sum-lite misc/wgsim /usr/local/bin\n",
            "install -p misc/blast2sam.pl misc/bowtie2sam.pl misc/export2sam.pl misc/interpolate_sam.pl misc/novo2sam.pl misc/plot-bamstats misc/psl2sam.pl misc/sam2vcf.pl misc/samtools.pl misc/seq_cache_populate.pl misc/soap2sam.pl misc/wgsim_eval.pl misc/zoom2sam.pl /usr/local/bin\n",
            "install -p -m 644 doc/samtools*.1 misc/wgsim.1 /usr/local/share/man/man1\n",
            "--2022-11-04 21:34:32--  https://github.com/lh3/minimap2/releases/download/v2.17/minimap2-2.17_x64-linux.tar.bz2\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/97612481/fafde000-6ec7-11e9-8f99-513f7e53bc2c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20221104%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20221104T213432Z&X-Amz-Expires=300&X-Amz-Signature=ccf596ab8e4bf61f7749054cce03a8a745c4d8025fb464427962769021cd2e49&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=97612481&response-content-disposition=attachment%3B%20filename%3Dminimap2-2.17_x64-linux.tar.bz2&response-content-type=application%2Foctet-stream [following]\n",
            "--2022-11-04 21:34:32--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/97612481/fafde000-6ec7-11e9-8f99-513f7e53bc2c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20221104%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20221104T213432Z&X-Amz-Expires=300&X-Amz-Signature=ccf596ab8e4bf61f7749054cce03a8a745c4d8025fb464427962769021cd2e49&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=97612481&response-content-disposition=attachment%3B%20filename%3Dminimap2-2.17_x64-linux.tar.bz2&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2100809 (2.0M) [application/octet-stream]\n",
            "Saving to: ‘minimap2-2.17_x64-linux.tar.bz2’\n",
            "\n",
            "minimap2-2.17_x64-l 100%[===================>]   2.00M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-11-04 21:34:32 (20.3 MB/s) - ‘minimap2-2.17_x64-linux.tar.bz2’ saved [2100809/2100809]\n",
            "\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!porechop -h "
      ],
      "metadata": {
        "id": "pWAtZNJytTCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/drive/MyDrive/ITS-Colab/EB-ITS-6-22-combined.fastq >> /content/drive/MyDrive/ITS-Colab/combined-ITS.fastq\n",
        "!cat /content/drive/MyDrive/ITS-Colab/EB-ITS-5-22.fastq >> /content/drive/MyDrive/ITS-Colab/combined-ITS.fastq"
      ],
      "metadata": {
        "id": "R8bC-DeDG4wn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload ITS fastq - Loaded on drive "
      ],
      "metadata": {
        "id": "WR1rkEWwvcux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp  /content/drive/MyDrive/ITS-Colab/scripts/adapter-barcode.py /usr/local/lib/python3.7/dist-packages/porechop/adapters.py"
      ],
      "metadata": {
        "id": "rpFhaHimxSkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!porechop -i /content/drive/MyDrive/ITS-Colab/combined-ITS.fastq -b /content/drive/MyDrive/ITS-Colab/ITS-demultiplex"
      ],
      "metadata": {
        "id": "PhPIrGsrtZdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Barcodes to pond\n",
        "\n",
        "BC01 = 404 \n",
        "BC02 = 405 \n",
        "BC03 = 923 "
      ],
      "metadata": {
        "id": "o7TtNCMFuOGu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combine fasta per site "
      ],
      "metadata": {
        "id": "wZQIue_dy7f0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!minimap2 -d /content/drive/MyDrive/ITS-Colab/silva.mmi /content/drive/MyDrive/ITS-Colab/SILVA_138.1_SSURef_NR99_tax_silva.fasta"
      ],
      "metadata": {
        "id": "vL2qc5w45gqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTktalinjz8h"
      },
      "source": [
        "# The Google Colab Environment does not have conda set, this would\n",
        "# ordinarily be the easies option to install these tools.\n",
        "\n",
        "!pip install git+https://github.com/rrwick/Porechop.git  # just so pomoxis will install cleanly\n",
        "!pip install medaka pomoxis aplanat intervaltree==3.0.2\n",
        "# install samtools from source\n",
        "!wget https://github.com/samtools/samtools/releases/download/1.10/samtools-1.10.tar.bz2\n",
        "!tar -xjf samtools-1.10.tar.bz2\n",
        "!cd samtools-1.10 && ./configure --prefix=/usr/local/ && make && make install\n",
        "!wget https://github.com/lh3/minimap2/releases/download/v2.17/minimap2-2.17_x64-linux.tar.bz2\n",
        "!tar -xjf minimap2-2.17_x64-linux.tar.bz2\n",
        "!cp /content/minimap2-2.17_x64-linux/minimap2 /usr/local/bin/\n",
        "!pip install requests "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!minimap2 -d /content/drive/MyDrive/ITS-Colab/UNITE/UNITE.mmi /content/drive/MyDrive/ITS-Colab/UNITE/sh_refs_qiime_ver8_dynamic_s_all_10.05.2021.fasta"
      ],
      "metadata": {
        "id": "-xUyCpxnjz8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/drive/MyDrive/ITS-Colab/ITS-demultiplex-30-9-22/\n",
        "!mv /content/drive/MyDrive/ITS-Colab/ITS-demultiplex/*fastq /content/drive/MyDrive/ITS-Colab/ITS-demultiplex-30-9-22/\n",
        "!mv BC01.fasta 404.fasta\n",
        "!mv BC02.fasta 405.fasta\n",
        "!mv BC03.fasta 923.fasta\n",
        "!mkdir /content/drive/MyDrive/ITS-Colab/ITS-demultiplex-30-9-22/scratch\n",
        "!mv /content/drive/MyDrive/ITS-Colab/ITS-demultiplex-30-9-22/none.fastq /content/drive/MyDrive/ITS-Colab/ITS-demultiplex-30-9-22/BC04.fastq /content/drive/MyDrive/ITS-Colab/ITS-demultiplex-30-9-22/scratch\n",
        "!mkdir /content/drive/MyDrive/ITS-Colab/ITS-demultiplex-30-9-22/fasta/\n",
        "!for f in /content/drive/MyDrive/ITS-Colab/ITS-demultiplex-30-9-22/*fastq ; do sed -n '1~4s/^@/>/p;2~4p' $f > /content/drive/MyDrive/ITS-Colab/ITS-demultiplex-30-9-22/fasta/$(basename $f .fastq).fasta ; done\n",
        "!mkdir /content/drive/MyDrive/ITS-Colab/sam/AbundanceTables-30-9-22\n",
        "!mkdir /content/drive/MyDrive/ITS-Colab/sam/samoutput-30-9-22"
      ],
      "metadata": {
        "id": "__17Nh-YkbtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can add a loop to command below to process all the ITS mock sequences at once. Also to get reads rather than species per genre you need to adjust the fasta headers so each one is unique"
      ],
      "metadata": {
        "id": "rhOUS_eTZFka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!minimap2 -K 5M -ax map-ont -L /content/drive/MyDrive/ITS-Colab/UNITE/UNITE.mmi /content/drive/MyDrive/ITS-Colab/ITS-demultiplex-30-9-22/fasta/404.fasta > 404.sam\n",
        "!minimap2 -K 5M -ax map-ont -L /content/drive/MyDrive/ITS-Colab/UNITE/UNITE.mmi /content/drive/MyDrive/ITS-Colab/ITS-demultiplex-30-9-22/fasta/405.fasta > 405.sam\n",
        "!minimap2 -K 5M -ax map-ont -L /content/drive/MyDrive/ITS-Colab/UNITE/UNITE.mmi /content/drive/MyDrive/ITS-Colab/ITS-demultiplex-30-9-22/fasta/923.fasta > 923.sam"
      ],
      "metadata": {
        "id": "J5YPlPrejz8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!for file in *.sam; do echo \"==> ${file} <==\"; grep -v '^@' \"${file}\" > \"${file}.output\"; done"
      ],
      "metadata": {
        "id": "qyBpFxZ0jz8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filter reads based on SAM header to keep only those that map and do not have additional SAM [flags](https://broadinstitute.github.io/picard/explain-flags.html).  \n",
        "\n",
        "This was not needed for bacteria, I wonder if its to do with the bacterial database being more complete/ appropriate? \n",
        "\n"
      ],
      "metadata": {
        "id": "N5KQge1Ojz8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/unite\n",
        "!cat /content/404.sam.output | awk -F'\\t' '$2 == 0||$2 == 256 {print $0;}' > /content/unite/404.sam.output\n",
        "!cat /content/405.sam.output | awk -F'\\t' '$2 == 0||$2 == 256 {print $0;}' > /content/unite/405.sam.output\n",
        "!cat /content/923.sam.output | awk -F'\\t' '$2 == 0||$2 == 256 {print $0;}' > /content/unite/923.sam.output"
      ],
      "metadata": {
        "id": "T9_t0aihjz8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/unite/*sam.output /content/drive/MyDrive/ITS-Colab/sam/samoutput-30-9-22/  "
      ],
      "metadata": {
        "id": "b-xavVLcuZST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Abundance tables \n",
        "To create abundcance tables we use more of the files from the SILVA DB and some adapted code from the [Puntseq](https://https://github.com/d-j-k/puntseq) project. If interested here is the related paper Urban L (2020), Freshwater monitoring by nanopore sequencing elife [link text](https://elifesciences.org/articles/61504)"
      ],
      "metadata": {
        "id": "uugP6fGxjz8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate table of reads per species\n",
        "import pandas as pd\n",
        "import io\n",
        "import os\n",
        "import requests\n",
        "import numpy as np\n",
        "# load all files from the SILVA database\n",
        "sild = pd.read_csv('/content/drive/MyDrive/ITS-Colab/UNITE/sh_taxonomy_qiime_ver8_dynamic_s_all_10.05.2021.txt', sep='\\t', header=None)\n",
        "sild.columns = ['taxid','tree']\n",
        "sild['ranks'] = [x.split(';')[-1:] for x in sild.tree.values]\n",
        "sild['tree'] = [x[0:-1] for x in sild.tree]\n",
        "sild.index = sild.taxid\n",
        "ranks = 'species'\n",
        "# choose dir of sam files\n",
        "dirc = '/content/unite' \n",
        "\n",
        "\n",
        "# create \n",
        "nr = 0\n",
        "for filename in os.listdir(dirc):\n",
        "    print(filename)\n",
        "    \n",
        "    try:\n",
        "        silva_10k = pd.read_csv('/content/unite/%s' %filename, \n",
        "                         sep='\\t', header=None, usecols = [0,2,4,13])\n",
        "    except: \n",
        "        continue\n",
        "    \n",
        "    silva_10k.columns = ['Read_ID', 'id','MS', 'ASs']\n",
        "    silva_10k['ASs'] = silva_10k['ASs'].astype('str')\n",
        "    silva_10k['AS'] = [x.split(':i:')[-1] for x in silva_10k['ASs'].values]\n",
        "    silva_10k.dropna(axis=0, subset=['AS'], inplace=True)\n",
        "    silva_10k['AS'] = silva_10k['AS'].astype('float')\n",
        "    mini = silva_10k[silva_10k['AS'] == silva_10k.groupby('Read_ID')['AS'].transform('max')]\n",
        "    mini = mini[['Read_ID', 'MS', 'AS','id']]              \n",
        "    mini.columns = ['read','score','as','id']\n",
        "    mini = mini[~mini.id.isnull()]  \n",
        "    mini['taxid'] =sild.ranks.loc[mini.id.values].values\n",
        "\n",
        "\n",
        "    if ranks == 'species':\n",
        "        mini['ranks'] = sild.ranks.loc[mini.id.values].values\n",
        "        mini.index = mini.read  \n",
        "        for i in mini.index[mini.duplicated(subset='read', keep=False)].unique():\n",
        "            minil = list(mini.loc[i].taxid.values)\n",
        "            if minil.count(minil[0]) != len(minil):\n",
        "                mini.drop(i)\n",
        "        mini.drop_duplicates(subset='read', keep='first', inplace=True)\n",
        "\n",
        "    mini['ranks']= [(x[0].strip(\"[]\")) for x in mini.ranks] \n",
        "    mini['ranks']= [(x.split(\"s__\")[1]) for x in mini.ranks]     #Current WORKS\n",
        "    mini2 = pd.DataFrame(mini.ranks.value_counts())\n",
        "\n",
        "    if nr==0:\n",
        "        minif = mini2.copy(deep=True)\n",
        "        minif.columns.values[nr] = filename.split('.')[0]\n",
        "    else:\n",
        "        minif = minif.merge(mini2, left_index=True, right_index=True, how='outer')\n",
        "        minif.columns.values[nr] = filename.split('.')[0]\n",
        "    nr = nr+1\n",
        "# describe all missing bacteria as absent\n",
        "minif = minif.fillna(0) \n",
        "  \n",
        "minif.to_csv('/content/minimap2_unite_species_%s.txt' %ranks, sep='\\t')\n"
      ],
      "metadata": {
        "id": "jdsAq4DSjz8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate table of reads per genus\n",
        "import pandas as pd\n",
        "import io\n",
        "import os\n",
        "import requests\n",
        "import numpy as np\n",
        "# load all files from the SILVA database\n",
        "sild = pd.read_csv('/content/drive/MyDrive/ITS-Colab/UNITE/sh_taxonomy_qiime_ver8_dynamic_s_all_10.05.2021.txt', sep='\\t', header=None)\n",
        "sild.columns = ['taxid','tree']\n",
        "sild['ranks'] = [x.split(';')[-2:-1] for x in sild.tree.values]\n",
        "sild['tree'] = [x[0:-1] for x in sild.tree]\n",
        "sild.index = sild.taxid\n",
        "ranks = 'genus'\n",
        "# choose dir of sam files\n",
        "dirc = '/content/unite' \n",
        "# create \n",
        "nr = 0\n",
        "for filename in os.listdir(dirc):\n",
        "    \n",
        "    try:\n",
        "        silva_10k = pd.read_csv('/content/unite/%s' %filename, \n",
        "                         sep='\\t', header=None, usecols = [0,2,4,13])\n",
        "    except: \n",
        "        continue\n",
        "    \n",
        "    silva_10k.columns = ['Read_ID', 'id','MS', 'ASs']\n",
        "    silva_10k['ASs'] = silva_10k['ASs'].astype('str')\n",
        "    silva_10k['AS'] = [x.split(':i:')[-1] for x in silva_10k['ASs'].values]\n",
        "    silva_10k.dropna(axis=0, subset=['AS'], inplace=True)\n",
        "    silva_10k['AS'] = silva_10k['AS'].astype('float')\n",
        "    mini = silva_10k[silva_10k['AS'] == silva_10k.groupby('Read_ID')['AS'].transform('max')]\n",
        "    mini = mini[['Read_ID', 'MS', 'AS','id']]              \n",
        "    mini.columns = ['read','score','as','id']\n",
        "    mini = mini[~mini.id.isnull()]  \n",
        "    mini['taxid'] =sild.ranks.loc[mini.id.values].values\n",
        "\n",
        "\n",
        "    if ranks == 'genus':\n",
        "        mini['ranks'] = sild.ranks.loc[mini.id.values].values\n",
        "        mini.index = mini.read  \n",
        "        for i in mini.index[mini.duplicated(subset='read', keep=False)].unique():\n",
        "            minil = list(mini.loc[i].taxid.values)\n",
        "            if minil.count(minil[0]) != len(minil):\n",
        "                mini.drop(i)\n",
        "        mini.drop_duplicates(subset='read', keep='first', inplace=True)\n",
        "\n",
        "    mini['ranks']= [(x[0].strip(\"[]\")) for x in mini.ranks] \n",
        "    mini['ranks']= [(x.split(\"g__\")[1]) for x in mini.ranks]     #Current WORKS\n",
        "    mini2 = pd.DataFrame(mini.ranks.value_counts())\n",
        "\n",
        "    if nr==0:\n",
        "        print('seen')\n",
        "        minif = mini2.copy(deep=True)\n",
        "        minif.columns.values[nr] = filename.split('.')[0]\n",
        "    else:\n",
        "        minif = minif.merge(mini2, left_index=True, right_index=True, how='outer')\n",
        "        minif.columns.values[nr] = filename.split('.')[0]\n",
        "\n",
        "    nr = nr+1\n",
        "# describe all missing bacteria as absent\n",
        "minif = minif.fillna(0) \n",
        "  \n",
        "minif.to_csv('/content/minimap2_unite_%s.txt' %ranks, sep='\\t')\n"
      ],
      "metadata": {
        "id": "bj2ne6Bajz8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate table of reads per family\n",
        "import pandas as pd\n",
        "import io\n",
        "import os\n",
        "import requests\n",
        "import numpy as np\n",
        "# load all files from the SILVA database\n",
        "sild = pd.read_csv('/content/drive/MyDrive/ITS-Colab/UNITE/sh_taxonomy_qiime_ver8_dynamic_s_all_10.05.2021.txt', sep='\\t', header=None)\n",
        "sild.columns = ['taxid','tree']\n",
        "sild['ranks'] = [x.split(';')[-4:-3] for x in sild.tree.values]\n",
        "sild['tree'] = [x[0:-1] for x in sild.tree]\n",
        "sild.index = sild.taxid\n",
        "ranks = 'order'\n",
        "# choose dir of sam files\n",
        "dirc = '/content/unite' \n",
        "# create \n",
        "nr = 0\n",
        "print('here')\n",
        "for filename in os.listdir(dirc):\n",
        "    print(filename) \n",
        "    \n",
        "    try:\n",
        "        silva_10k = pd.read_csv('/content/unite/%s' %filename, sep='\\t', header=None, usecols = [0,2,4,13])\n",
        "    except: \n",
        "        continue\n",
        "    \n",
        "    silva_10k.columns = ['Read_ID', 'id','MS', 'ASs']\n",
        "    silva_10k['ASs'] = silva_10k['ASs'].astype('str')\n",
        "    silva_10k['AS'] = [x.split(':i:')[-1] for x in silva_10k['ASs'].values]\n",
        "    silva_10k.dropna(axis=0, subset=['AS'], inplace=True)\n",
        "    silva_10k['AS'] = silva_10k['AS'].astype('float')\n",
        "    mini = silva_10k[silva_10k['AS'] == silva_10k.groupby('Read_ID')['AS'].transform('max')]\n",
        "    mini = mini[['Read_ID', 'MS', 'AS','id']]              \n",
        "    mini.columns = ['read','score','as','id']\n",
        "    mini = mini[~mini.id.isnull()]  \n",
        "    mini['taxid'] =sild.ranks.loc[mini.id.values].values\n",
        "\n",
        "\n",
        "    if ranks == 'order':\n",
        "        mini['ranks'] = sild.ranks.loc[mini.id.values].values\n",
        "        mini.index = mini.read  \n",
        "        for i in mini.index[mini.duplicated(subset='read', keep=False)].unique():\n",
        "            minil = list(mini.loc[i].taxid.values)\n",
        "            if minil.count(minil[0]) != len(minil):\n",
        "                mini.drop(i)\n",
        "        mini.drop_duplicates(subset='read', keep='first', inplace=True)\n",
        "\n",
        "    mini['ranks']= [(x[0].strip(\"[]\")) for x in mini.ranks] \n",
        "    mini['ranks']= [(x.split(\"o__\")[1]) for x in mini.ranks]     #Current WORKS\n",
        "    mini2 = pd.DataFrame(mini.ranks.value_counts())\n",
        "\n",
        "    if nr==0:\n",
        "        minif = mini2.copy(deep=True)\n",
        "        minif.columns.values[nr] = filename.split('.')[0]\n",
        "    else:\n",
        "        minif = minif.merge(mini2, left_index=True, right_index=True, how='outer')\n",
        "        minif.columns.values[nr] = filename.split('.')[0]\n",
        "\n",
        "    nr = nr+1\n",
        "# describe all missing bacteria as absent\n",
        "minif = minif.fillna(0) \n",
        "  \n",
        "minif.to_csv('/content/minimap2_unite_%s.txt' %ranks, sep='\\t')\n"
      ],
      "metadata": {
        "id": "z5Ag9Zbg-JvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate table of reads per genus\n",
        "import pandas as pd\n",
        "import io\n",
        "import os\n",
        "import requests\n",
        "import numpy as np\n",
        "# load all files from the SILVA database\n",
        "sild = pd.read_csv('/content/drive/MyDrive/ITS-Colab/UNITE/sh_taxonomy_qiime_ver8_dynamic_s_all_10.05.2021.txt', sep='\\t', header=None)\n",
        "sild.columns = ['taxid','tree']\n",
        "sild['ranks'] = [x.split(';')[-6:-5] for x in sild.tree.values]\n",
        "sild['tree'] = [x[0:-1] for x in sild.tree]\n",
        "sild.index = sild.taxid\n",
        "ranks = 'phylum'\n",
        "# choose dir of sam files\n",
        "dirc = '/content/' \n",
        "# create \n",
        "nr = 0\n",
        "for filename in os.listdir(dirc):\n",
        "    \n",
        "    try:\n",
        "        silva_10k = pd.read_csv('/content/%s' %filename, \n",
        "                         sep='\\t', header=None, usecols = [0,2,4,13])\n",
        "    except: \n",
        "        continue\n",
        "    \n",
        "    silva_10k.columns = ['Read_ID', 'id','MS', 'ASs']\n",
        "    silva_10k['ASs'] = silva_10k['ASs'].astype('str')\n",
        "    silva_10k['AS'] = [x.split(':i:')[-1] for x in silva_10k['ASs'].values]\n",
        "    silva_10k.dropna(axis=0, subset=['AS'], inplace=True)\n",
        "    silva_10k['AS'] = silva_10k['AS'].astype('float')\n",
        "    mini = silva_10k[silva_10k['AS'] == silva_10k.groupby('Read_ID')['AS'].transform('max')]\n",
        "    mini = mini[['Read_ID', 'MS', 'AS','id']]              \n",
        "    mini.columns = ['read','score','as','id']\n",
        "    mini = mini[~mini.id.isnull()]  \n",
        "    mini['taxid'] =sild.ranks.loc[mini.id.values].values\n",
        "\n",
        "\n",
        "    if ranks == 'phylum':\n",
        "        mini['ranks'] = sild.ranks.loc[mini.id.values].values\n",
        "        mini.index = mini.read  \n",
        "        for i in mini.index[mini.duplicated(subset='read', keep=False)].unique():\n",
        "            minil = list(mini.loc[i].taxid.values)\n",
        "            if minil.count(minil[0]) != len(minil):\n",
        "                mini.drop(i)\n",
        "        mini.drop_duplicates(subset='read', keep='first', inplace=True)\n",
        "\n",
        "    mini['ranks']= [(x[0].strip(\"[]\")) for x in mini.ranks] \n",
        "    mini['ranks']= [(x.split(\"p__\")[1]) for x in mini.ranks]     #Current WORKS\n",
        "    mini2 = pd.DataFrame(mini.ranks.value_counts())\n",
        "\n",
        "    if nr==0:\n",
        "        minif = mini2.copy(deep=True)\n",
        "        minif.columns.values[nr] = filename.split('.')[0]\n",
        "    else:\n",
        "        minif = minif.merge(mini2, left_index=True, right_index=True, how='outer')\n",
        "        minif.columns.values[nr] = filename.split('.')[0]\n",
        "\n",
        "    nr = nr+1\n",
        "# describe all missing bacteria as absent\n",
        "minif = minif.fillna(0) \n",
        "  \n",
        "minif.to_csv('/content/minimap2_unite_%s.txt' %ranks, sep='\\t')\n"
      ],
      "metadata": {
        "id": "x1gAIgtD-oGn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}