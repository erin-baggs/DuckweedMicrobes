{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/erin-baggs/DuckweedMicrobes/blob/main/IndivSamples_16S_Data_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Google co-lab notebook for 16S rRNA amplicon data processing\n",
        "\n",
        "Erin Baggs erinbaggs95@berkeley.edu\n",
        "\n",
        "Adapted for duckweed  from https://github.com/d-j-k/puntseq Lara Urban, Andre Holzer, J Jotautas Baronas, Michael Hall, Philipp Braeuninger-Weimer, Michael J Scherm, Daniel J Kunz, Surangi N Perera, Daniel E Martin-Herranz, Edward T Tipper, Susannah J Salter, and Maximilian R Stammnitz"
      ],
      "metadata": {
        "id": "o6peuUAsLyoP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is Google Co-Lab**\n",
        "\n",
        "Colab is a free Jupyter notebook environment that runs entirely in the cloud. Jupyter notebooks can integrate many programming languages like Python, PHP, R, C#. Allowing you to create and share documents that contain live code, equations, visualizations, and narrative text.\n",
        "\n",
        "[Background](https://colab.research.google.com/?utm_source=scs-index) on using google colab from google if interested. \n",
        "\n"
      ],
      "metadata": {
        "id": "37hYTq6lGQ-X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Cell Basics** \n",
        "This is a **text cell**. You can **double-click** to edit these cells with your own notes. Text cells\n",
        "use markdown syntax. To learn more, see our [markdown\n",
        "guide](/notebooks/markdown_guide.ipynb).You can add either text or code cells using the the top toolbar. "
      ],
      "metadata": {
        "id": "NasVB3oHKG49"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code Cell Basics**\n",
        "\n",
        "Below is a **code cell**. Once the toolbar button indicates CONNECTED, click in the cell to select it and execute the contents in the following ways:\n",
        "\n",
        "Click the Play icon in the left gutter of the cell;\n",
        "Type Cmd/Ctrl+Enter to run the cell in place;\n",
        "Type Shift+Enter to run the cell and move focus to the next cell (adding one if none exists); or\n",
        "Type Alt+Enter to run the cell and insert a new code cell immediately below it.\n",
        "There are additional options for running some or all cells in the Runtime menu.\n",
        "\n",
        "You can edit the code if you double-click on the cell. You can try this with the cell below by changing the print message. If you want to add a note (not code to run) directly to the code then use `#` to tell colab to ignore the rest of the line. \n",
        "\n",
        "To get the most out of the workshop try to edit the code where possible to check your understanding of what is going on. \n",
        "\n",
        "If your changes prevent the code from running you will no longer see a tick at the side of the code cell and the output will likely contain a series of warning or error messages. "
      ],
      "metadata": {
        "id": "TSIYRvryKEXR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we start so you can keep any changes you make a copy of this notebook in google drive.  File -> Save a copy in drive.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hdZP5R_M2XU-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Where is our data from?**\n",
        "\n",
        "For background on the data we are using today check out the bioRxiv [doc]<insert link>"
      ],
      "metadata": {
        "id": "iwOxlQNIn_dV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Getting started with Google Co-Lab**\n",
        "\n",
        "\n",
        "To start we need to link our use of this notebook to our google drive. To do this we will run the grey code block below by clicking on the block and pressing the play button that appears or by clicking on it and pressing shift + enter. \n",
        "\n",
        "It will first ask you trust to agree that you want to run the notebook as its not authored by google but our lab.\n",
        "\n",
        "This will generate a pop-up that asks you if you want google colab to access drive. Select your UC Berkeley google drive account and then on the next pop-up I normally give it all permissions available and press allow. Once approved your drive should start loading. \n"
      ],
      "metadata": {
        "id": "sHjpNs4MleCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "FL7swq1fi0_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Get data for tutorial**\n",
        "\n",
        "To access the data needed for this tutorial we need to download [SILVA](https://www.arb-silva.de/) database files (tax_slv_ssu_138.1.txt, SILVA_138.1_SSURef_NR99_tax_silva.fasta, seqid2taxid.map, nodes.dmp, names.dmp).\n",
        "\n",
        "The demultiplexed fasta reads which allow you to skip to **Mapping to reference database** can be found on ncbi PRJNA785658 (SRR18059370-SRR18059372). \n",
        "\n",
        "Versions of barcode demultiplex scripts used in processing raw nanopore reads can be found on [github](https://github.com/krasileva-group/Duckweed-Microbiome.git).  \n",
        "\n",
        "Once you have downloaded the data put it in your google drive or load into the colab session. \n"
      ],
      "metadata": {
        "id": "czCKKPZOjDXn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Processing raw nanopore reads (skip if have fasta)"
      ],
      "metadata": {
        "id": "YU-9MvdkcETP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall --yes gdown # After running this line, restart Colab runtime."
      ],
      "metadata": {
        "id": "MnHk6vqtC7G9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order for the colab session to forget the old version of the software gdown we need to go to Runtime at the top tool bar and press the option 'Restart Runtime' before we continue with the next cell. "
      ],
      "metadata": {
        "id": "hPt9BOTOEfDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown -U --no-cache-dir\n",
        "\n",
        "import gdown\n",
        "\n",
        "url = '<insert your url to reads>'\n",
        "\n",
        "gdown.download_folder(url)\n",
        "!mv /content/16SColab /content/drive/MyDrive"
      ],
      "metadata": {
        "id": "J34yGsSYCqSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setup output directories for output**\n",
        "In order to have somewhere to  put the reads we first run the `mkdir` command in the cell below. This creates a folder in which we can put our location-based reads. \n"
      ],
      "metadata": {
        "id": "iRyltrG_nx4r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are prefixing our commands with an `!` to the tell the workbook to run this using a shell rather than python. \n",
        "We are making directories to keep our data organized in our drives. "
      ],
      "metadata": {
        "id": "Ap9wFYl0FO_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/drive/MyDrive/16SColab/sam/\n",
        "!mkdir /content/drive/MyDrive/16SColab/demultiplexed/\n",
        "!mkdir /content/drive/MyDrive/16SColab/fasta/\n",
        "!mkdir /content/drive/MyDrive/16SColab/abundanceTables/\n",
        "!mkdir /content/drive/MyDrive/16SColab/samlog\n",
        "!mkdir /content/drive/MyDrive/16SColab/samsorted\n",
        "!mkdir /content/drive/MyDrive/16SColab/samoutput"
      ],
      "metadata": {
        "id": "boyZCwo-oFTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install tools needed for data processing**\n",
        "\n",
        "Google Colab doesnt suppport conda environments that are often used to load tools needed. Instead we will use pip install together with downloads from github to access needed tools."
      ],
      "metadata": {
        "id": "Hfb2k_jrmUKq"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9a7MtewFIAH"
      },
      "source": [
        "!pip install git+https://github.com/rrwick/Porechop.git  # just so pomoxis will install cleanly\n",
        "!pip install medaka pomoxis aplanat intervaltree==3.0.2\n",
        "# install samtools from source\n",
        "!wget https://github.com/samtools/samtools/releases/download/1.10/samtools-1.10.tar.bz2\n",
        "!tar -xjf samtools-1.10.tar.bz2\n",
        "!cd samtools-1.10 && ./configure --prefix=/usr/local/ && make && make install\n",
        "!wget https://github.com/lh3/minimap2/releases/download/v2.17/minimap2-2.17_x64-linux.tar.bz2\n",
        "!tar -xjf minimap2-2.17_x64-linux.tar.bz2\n",
        "!cp /content/minimap2-2.17_x64-linux/minimap2 /usr/local/bin/\n",
        "!pip install requests \n",
        "!pip install zenodo-get\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have installed tools I like to double check that they work. One way to do this is to ask for the tool help page which will tell you what options are available. \n",
        "\n",
        "Once cells run you can clear the output from your screen so you dont have to scroll through them. You can do this by hovering your cursor over the top of the square brackets and then draw it down until a cross appears.  "
      ],
      "metadata": {
        "id": "m9FU7OE3mwYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "d1ieB1r_8VVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!porechop -h "
      ],
      "metadata": {
        "id": "pWAtZNJytTCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we know Porechop has installed properly we want to find exactly were it is installed as we need to customize some of the code to convert it from removing Oxford nanopore barcode to the PacBio barcodes we used in our experiment. "
      ],
      "metadata": {
        "id": "WR1rkEWwvcux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list -v | grep [Pp]orechop"
      ],
      "metadata": {
        "id": "bFsJmVzhwDfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The location is given in the third column as a list of folders (aka directories) seperated by slashes. If the location printed from cell above is different to the one below then copy and paste the location in the third column followed immediatley by a slash and the name of the tool as printed in the first column. We can test this by using `ls` to list the files inside of the porechop folder. If you get a list containing some .py files (thesen are python scripts) then it worked. "
      ],
      "metadata": {
        "id": "2vBGCcJXpOMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /usr/local/lib/python3.7/dist-packages/porechop/"
      ],
      "metadata": {
        "id": "mtZNaF7Pw5sj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can replace the native barcode file in porechop with the pacbio barcode version that is part of the data you have put in your drive. \n"
      ],
      "metadata": {
        "id": "TdpuFf_WpNxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp  /content/drive/MyDrive/16SColab/scripts/adapter-barcode-fwd.py /usr/local/lib/python3.7/dist-packages/porechop/adapters.py"
      ],
      "metadata": {
        "id": "rpFhaHimxSkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To save us some work we are providing one fastq file which contains all the reads from the two runs on   consecutive days  using a single library prep . \n",
        "\n",
        "Porechop is able to find and sort reads by the 5' and 3' barcodes that allow us to pool multiple samples into one sequencing run. As well as barcodes porechop identifies and removes ONT adaptors sequences to prevent them leading to mismapping. \n",
        "\n",
        "As our samples have unique fwd and rev barcodes we first have to seperate the fwd barcodes and then we re-run porechop with the rev barcodes to identify the combination of barcodes for each sample. "
      ],
      "metadata": {
        "id": "tR490Cgdp6OV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the whole dataset takes >1hr to demultiplex I would sugggest to instead run the script on a subsample of the data (unless you want to run it whilst you are doing something else and not just during class**). You can adjust the number of reads to include in your subsample. As fastq files have 4 lines per read I would suggest to try a multiple of 4. To adjust double click on the code block below and change the number after `-n ` it will be interesting to see affects of sample sizes on the outcomes.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# head -n \n",
        "```\n",
        "\n",
        "\n",
        "**Notes for running on full dataset not subsample \n",
        "Do not run cell below instead skip to porechop command and swap the input filename from flongle-subsample.fastq to flongle-combined.fastq\n"
      ],
      "metadata": {
        "id": "eOriCQqz2o--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 181048 /content/drive/MyDrive/16SColab/RawReads/flongle-combined.fastq > /content/drive/MyDrive/16SColab/RawReads/flongle-subsample.fastq"
      ],
      "metadata": {
        "id": "e_mgcaJt2OEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!porechop -i /content/drive/MyDrive/16SColab/RawReads/flongle-subsample.fastq -b /content/drive/MyDrive/16SColab/flongle-demult1"
      ],
      "metadata": {
        "id": "PhPIrGsrtZdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp  /content/drive/MyDrive/16SColab/scripts/adapter-barcode-rev.py /usr/local/lib/python3.7/dist-packages/porechop/adapters.py"
      ],
      "metadata": {
        "id": "piPIegphy79z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have identified forward barcodes we are now going to search for the reverse ones, to do this we need to give porechop a new barcode list which we do with the copy command \n",
        "\n",
        "```\n",
        "cp\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7DaahaD76-vk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we need to create somewhere to put the reverse demultiplexed reads. To do this we use \n",
        "\n",
        "```\n",
        "mkdir\n",
        "```\n",
        "to make a new folder/directory for our second demultiplexing run and sub directories for each of the fwd barcodes. \n"
      ],
      "metadata": {
        "id": "FdARejwI7Usl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/drive/MyDrive/16SColab/flongle-demult2/\n",
        "!for i in /content/drive/MyDrive/16SColab/flongle-demult1/BC*; do mkdir /content/drive/MyDrive/16SColab/flongle-demult2/$(basename $i .fastq) ;done\n",
        "!for i in /content/drive/MyDrive/16SColab/flongle-demult1/BC*; do mv $i /content/drive/MyDrive/16SColab/flongle-demult2/$(basename $i .fastq) ;done"
      ],
      "metadata": {
        "id": "HBvH1u7LvkHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are ready to run the reverse demultiplex. This time we loop through each of the first demultiplex output which has seperated the reads into 5 groups based on barcodes"
      ],
      "metadata": {
        "id": "iSNm0w-g7shS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!for i in /content/drive/MyDrive/16SColab/flongle-demult2/BC*; do porechop -i $i -b /content/drive/MyDrive/16SColab/flongle-demult2/$(basename $i .fastq);done"
      ],
      "metadata": {
        "id": "E27Zyc3EzRg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assign demultiplexed reads to the location they are from**\n",
        "\n",
        "\n",
        "Below is the breakdown of which barcodes can be used to identify reads from a given location\n",
        "BC01  & [BC02 + BC05] = Location 404 \n",
        "[BC02 + BC06-8] & [BC03 + BC05-6]= Location 405 \n",
        "[BC03 + BC07-8] & [BC04]= Loaction 923 \n",
        "\n",
        "To join all reads from the same location into a single file we use \n",
        "\n",
        "```\n",
        "cat indv_file_name >> combined_file_name \n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "o7TtNCMFuOGu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#mkdir /content/drive/MyDrive/16SColab/demultiplexed-indiv/\n",
        "!mkdir /content/drive/MyDrive/16SColab/fasta-indiv\n",
        "!mkdir /content/drive/MyDrive/16SColab/sam-indiv\n",
        "!mkdir /content/drive/MyDrive/16SColab/samsorted-indiv\n",
        "!mkdir /content/drive/MyDrive/16SColab/samoutput-indiv\n",
        "!mkdir /content/drive/MyDrive/16SColab/abundanceTables-indiv"
      ],
      "metadata": {
        "id": "S_XUjE1b1yG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/drive/MyDrive/16SColab/flongle-demult2/BC01/BC05.fastq >> /content/drive/MyDrive/16SColab/demultiplexed-indiv/404-1.fastq\n",
        "!cat /content/drive/MyDrive/16SColab/flongle-demult2/BC01/BC06.fastq >> /content/drive/MyDrive/16SColab/demultiplexed-indiv/404-2.fastq\n",
        "!cat /content/drive/MyDrive/16SColab/flongle-demult2/BC01/BC07.fastq >> /content/drive/MyDrive/16SColab/demultiplexed-indiv/404-3.fastq\n",
        "!cat /content/drive/MyDrive/16SColab/flongle-demult2/BC01/BC08.fastq >> /content/drive/MyDrive/16SColab/demultiplexed-indiv/404-4.fastq\n",
        "!cat /content/drive/MyDrive/16SColab/flongle-demult2/BC02/BC05.fastq >> /content/drive/MyDrive/16SColab/demultiplexed-indiv/404-5.fastq\n",
        "\n",
        "!cat /content/drive/MyDrive/16SColab/flongle-demult2/BC02/BC06.fastq >> /content/drive/MyDrive/16SColab/demultiplexed-indiv/405-1.fastq\n",
        "!cat /content/drive/MyDrive/16SColab/flongle-demult2/BC02/BC07.fastq >> /content/drive/MyDrive/16SColab/demultiplexed-indiv/405-2.fastq\n",
        "!cat /content/drive/MyDrive/16SColab/flongle-demult2/BC02/BC08.fastq >> /content/drive/MyDrive/16SColab/demultiplexed-indiv/405-3.fastq\n",
        "!cat /content/drive/MyDrive/16SColab/flongle-demult2/BC03/BC05.fastq >> /content/drive/MyDrive/16SColab/demultiplexed-indiv/405-4.fastq\n",
        "!cat /content/drive/MyDrive/16SColab/flongle-demult2/BC03/BC06.fastq >> /content/drive/MyDrive/16SColab/demultiplexed-indiv/405-5.fastq\n",
        "\n",
        "!cat /content/drive/MyDrive/16SColab/flongle-demult2/BC03/BC07.fastq >> /content/drive/MyDrive/16SColab/demultiplexed-indiv/923-1.fastq\n",
        "!cat /content/drive/MyDrive/16SColab/flongle-demult2/BC03/BC08.fastq >> /content/drive/MyDrive/16SColab/demultiplexed-indiv/923-2.fastq\n",
        "!cat /content/drive/MyDrive/16SColab/flongle-demult2/BC04/BC05.fastq >> /content/drive/MyDrive/16SColab/demultiplexed-indiv/923-3.fastq\n",
        "!cat /content/drive/MyDrive/16SColab/flongle-demult2/BC04/BC06.fastq >> /content/drive/MyDrive/16SColab/demultiplexed-indiv/923-4.fastq\n",
        "!cat /content/drive/MyDrive/16SColab/flongle-demult2/BC04/BC07.fastq >> /content/drive/MyDrive/16SColab/demultiplexed-indiv/923-5.fastq\n"
      ],
      "metadata": {
        "id": "NbI-yqqPybSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        " # Mapping to reference database \n",
        " \n",
        " We map to the [SILVA](https://www.arb-silva.de/) database to identify the genus of bacteria from which read is derived.\n",
        "\n",
        "To download the required database files from SILVA from [here](https://www.arb-silva.de/no_cache/download/archive/release_138_1/Exports/)\n",
        "\n",
        "The first step is to create a minimizer index (.mmi) of the reference sequence database. This enables faster and lower memory usage upon alignment. "
      ],
      "metadata": {
        "id": "jWQUlXtA336K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!minimap2 -d /content/drive/MyDrive/16SColab/SILVA/silva.mmi /content/drive/MyDrive/16SColab/SILVA/SILVA_138.1_SSURef_NR99_tax_silva.fasta"
      ],
      "metadata": {
        "id": "vL2qc5w45gqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we create a folder for fasta versions of our reads which will not have the quality score information. Then using sed we can create the fasta from the fastq and place them in the new directory. We use a loop to send f which is a file ending in .fastq through the pipeline. This allows us to run the line of code once rather than three times. \n",
        "\n",
        "```\n",
        " for f in *.fastq ; do \n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8bV22VqWySKp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!for f in /content/drive/MyDrive/16SColab/demultiplexed-indiv/*fastq ; do sed -n '1~4s/^@/>/p;2~4p' $f > /content/drive/MyDrive/16SColab/fasta-indiv/$(basename $f .fastq).fasta ; done"
      ],
      "metadata": {
        "id": "xkrxFhI555qW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we have the fasta files we can use the tool minimap to align them to our reference database. Using `-ax` option we tell the aligner these are ONT reads this allows it to take into account the error profile of ONT reads. The `-K` option allows us to constrain memory usage to prevent the process being killed due to requiring too much resources.\n",
        "\n"
      ],
      "metadata": {
        "id": "7EB8psc1zIB5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!for f in /content/drive/MyDrive/16SColab/fasta-indiv/*; do  minimap2 -K 5M -ax map-ont -L /content/drive/MyDrive/16SColab/SILVA/silva.mmi $f > /content/drive/MyDrive/16SColab/sam-indiv/$(basename $f .fasta).sam ;done"
      ],
      "metadata": {
        "id": "if9b45u855YA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert SAM files to abundance tables \n",
        "The next step is to convert SAM files to format useable for converting creating abundance tables. At the start of SAM files are header lines which start with \n",
        "\n",
        "```\n",
        "@\n",
        "```\n",
        "and contain information about the sequences and program used to generate SAM files.\n",
        "\n",
        "After the header lines the lines represent information about the alignment of a query sequence to the reference (ie your mmi database). Each column has a specific meaning.\n",
        "\n",
        "We can use the program samtools to reorder our sequences alphabetically then remove the headers as we do not need these for our purpose. "
      ],
      "metadata": {
        "id": "VgUOPKyYnqB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!for file in /content/drive/MyDrive/16SColab/sam-indiv/*.sam; do samtools sort -@ 2 -O sam -o /content/drive/MyDrive/16SColab/samsorted-indiv/$(basename $file).sorted $file 2> /content/drive/MyDrive/16SColab/samlog/$(basename $file).indiv.log ; done\n",
        "!for file in /content/drive/MyDrive/16SColab/samsorted-indiv/*.sorted; do echo \"==> ${file} <==\"; grep -v '^@' \"${file}\" > /content/drive/MyDrive/16SColab/samoutput-indiv/$(basename $file).output; done"
      ],
      "metadata": {
        "id": "c1x2CPIX4ViN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Abundance Tables \n",
        "\n",
        "Now we have aligned our reads to the reference database we need to assign each read to a taxa based on the best alignment. At the same time we need to keep a count of how many reads are assigned to a given taxa at each site. \n",
        "\n",
        "We can do this process repeatedly investigating different taxonomic levels. \n",
        "\n",
        "To do this we will use a python script that is below. \n",
        "\n",
        "As it is good pracitce to try to read and understand the steps in the script, I would suggest that once you have your abundance tables which should save to your Gdrive in 16S Colab -> abundanceTables folder you create a copy with a different name to save them for next week . Then you can play around with the python scripts below to help your understanding. E.G. you cangoogle commands you are unfamiliar with,edit names of variables,  add print statements to objects that you are unsure what they contain\n",
        "\n",
        "```\n",
        "print(object_name)\n",
        "```\n",
        "\n",
        ". Anything you print will be output to the box below the script. If you get errors googling these can help you solve them.  If you make any changes that you cant fix simply use Edit -> Undo .. to get back to the original version. "
      ],
      "metadata": {
        "id": "tyosTFqExqcA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Generate table of reads per genus"
      ],
      "metadata": {
        "id": "In6vNmVX-DLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "import os\n",
        "import requests\n",
        "import numpy as np\n",
        "# load all files from the SILVA database\n",
        "minit = pd.read_csv('/content/drive/MyDrive/16SColab/SILVA/seqid2taxid.map', sep='\\t', index_col=0, header=None)\n",
        "sild = pd.read_csv('/content/drive/MyDrive/16SColab/SILVA/tax_slv_ssu_138.1.txt', sep='\\t', header=None)\n",
        "sild.columns = ['tree','taxid','level','3','4']\n",
        "sild['ranks'] = [x.split(';')[-2] for x in sild.tree.values]\n",
        "sild['tree'] = [x[0:-1] for x in sild.tree]\n",
        "sild.index = sild.taxid\n",
        "namesdmp = pd.read_csv('/content/drive/MyDrive/16SColab/SILVA/names.dmp', sep='\\t', index_col=0, header=None)\n",
        "# choose rank and month\n",
        "ranks = 'genus'\n",
        "# choose dir of sam files\n",
        "dirc = '/content/drive/MyDrive/16SColab/samoutput-indiv/' \n",
        "# create \n",
        "nr = 0\n",
        "for filename in os.listdir(dirc):\n",
        "    print(filename)\n",
        "    \n",
        "    try:\n",
        "        silva_10k = pd.read_csv('/content/drive/MyDrive/16SColab/samoutput-indiv/%s' %filename, \n",
        "                         sep='\\t', header=None, usecols = [0,2,4,13])\n",
        "    except: \n",
        "        continue\n",
        "    \n",
        "    silva_10k.columns = ['Read_ID', 'id','MS', 'ASs']\n",
        "    silva_10k['ASs'] = silva_10k['ASs'].astype('str')\n",
        "    silva_10k['AS'] = [x.split(':i:')[-1] for x in silva_10k['ASs'].values]\n",
        "    silva_10k.dropna(axis=0, subset=['AS'], inplace=True)\n",
        "    silva_10k['AS'] = silva_10k['AS'].astype('float')\n",
        "    mini = silva_10k[silva_10k['AS'] == silva_10k.groupby('Read_ID')['AS'].transform('max')]\n",
        "    mini = mini[['Read_ID', 'MS', 'AS','id']]              \n",
        "    mini.columns = ['read','score','as','id']\n",
        "    mini = mini[~mini.id.isnull()]\n",
        "    mini['taxid'] = minit.loc[mini.id.values].values\n",
        "    mini['tree'] = sild.loc[mini.taxid.values]['tree'].values\n",
        "    mini['level'] = sild.loc[mini.taxid.values]['level'].values\n",
        "    mini['phylum'] = [x.split(';')[0] for x in mini.tree]\n",
        "    print(mini.phylum.unique())\n",
        "    # if genus\n",
        "    if ranks == 'genus':\n",
        "        mini = mini[mini.level=='genus'] \n",
        "        mini['ranks'] = namesdmp.loc[mini.taxid][2].values \n",
        "        mini.index = mini.read  \n",
        "        for i in mini.index[mini.duplicated(subset='read', keep=False)].unique():\n",
        "            minil = list(mini.loc[i].taxid.values)\n",
        "            if minil.count(minil[0]) != len(minil):\n",
        "                mini.drop(i)\n",
        "        mini.drop_duplicates(subset='read', keep='first', inplace=True)\n",
        "    # if family\n",
        "    if ranks == 'family':\n",
        "        mini = mini[(mini.level=='genus')|(mini.level=='family')] # 9349\n",
        "        mini['ranks'] = namesdmp.loc[mini.taxid][2].values   \n",
        "        mini.index = mini.read\n",
        "        for i in mini.index[mini.duplicated(subset='read', keep=False)].unique():\n",
        "            minil = list(mini.loc[i].taxid.values)\n",
        "            if minil.count(minil[0]) != len(minil):\n",
        "                minil2 = [x.split(';')[-2] for x in mini.loc[i].tree.values]\n",
        "                if minil2.count(minil2[0]) == len(minil2):\n",
        "                    #print('3')\n",
        "                    mini['ranks'].loc[i] = minil2[0]\n",
        "                else: \n",
        "                    mini.drop(i)\n",
        "        mini.drop_duplicates(subset='read', keep='first', inplace=True)           \n",
        "        mini['genuss'] = [sild.loc[x].tree.split(';')[-2] for x in mini.taxid]\n",
        "        mini['ranks'][mini.level=='genus'] = mini['genuss'][mini.level=='genus']\n",
        "    mini2 = pd.DataFrame(mini.ranks.value_counts())\n",
        "    if nr==0:\n",
        "        #print('4')\n",
        "        minif = mini2.copy(deep=True)\n",
        "        minif.columns.values[nr] = filename.split('.')[0]\n",
        "    else:\n",
        "        minif = minif.merge(mini2, left_index=True, right_index=True, how='outer')\n",
        "        minif.columns.values[nr] = filename.split('.')[0]\n",
        "    nr = nr+1\n",
        "# describe all missing bacteria as absent\n",
        "minif = minif.fillna(0) \n",
        "#Above causes warning cant figure out best way to unchain\n",
        "# save the final txt files  \n",
        "minif.to_csv('/content/drive/MyDrive/16SColab/abundanceTables-indiv/minimap2_k12_%s.txt' %ranks, sep='\\t')"
      ],
      "metadata": {
        "id": "sd0D8lB0-NWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate reads per family table "
      ],
      "metadata": {
        "id": "RkEQ_HuJ9wi1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "import os\n",
        "import requests\n",
        "import numpy as np\n",
        "# load all files from the SILVA database\n",
        "minit = pd.read_csv('/content/drive/MyDrive/16SColab/SILVA/seqid2taxid.map', sep='\\t', index_col=0, header=None)\n",
        "sild = pd.read_csv('/content/drive/MyDrive/16SColab/SILVA/tax_slv_ssu_138.1.txt', sep='\\t', header=None)\n",
        "sild.columns = ['tree','taxid','level','3','4']\n",
        "sild['ranks'] = [x.split(';')[-2] for x in sild.tree.values]\n",
        "sild['tree'] = [x[0:-1] for x in sild.tree]\n",
        "sild.index = sild.taxid\n",
        "namesdmp = pd.read_csv('/content/drive/MyDrive/16SColab/SILVA/names.dmp', sep='\\t', index_col=0, header=None)\n",
        "# choose rank and month\n",
        "ranks = 'family'\n",
        "# choose dir of sam files\n",
        "dirc = '/content/drive/MyDrive/16SColab/samoutput-indiv/' \n",
        "# create \n",
        "nr = 0\n",
        "for filename in os.listdir(dirc):\n",
        "    print(filename)\n",
        "    \n",
        "    try:\n",
        "        silva_10k = pd.read_csv('/content/drive/MyDrive/16SColab/samoutput-indiv/%s' %filename, \n",
        "                         sep='\\t', header=None, usecols = [0,2,4,13])\n",
        "    except: \n",
        "        continue\n",
        "    \n",
        "    silva_10k.columns = ['Read_ID', 'id','MS', 'ASs']\n",
        "    silva_10k['ASs'] = silva_10k['ASs'].astype('str')\n",
        "    silva_10k['AS'] = [x.split(':i:')[-1] for x in silva_10k['ASs'].values]\n",
        "    silva_10k.dropna(axis=0, subset=['AS'], inplace=True)\n",
        "    silva_10k['AS'] = silva_10k['AS'].astype('float')\n",
        "    mini = silva_10k[silva_10k['AS'] == silva_10k.groupby('Read_ID')['AS'].transform('max')]\n",
        "    mini = mini[['Read_ID', 'MS', 'AS','id']]              \n",
        "    mini.columns = ['read','score','as','id']\n",
        "    mini = mini[~mini.id.isnull()]\n",
        "    mini['taxid'] = minit.loc[mini.id.values].values\n",
        "    mini['tree'] = sild.loc[mini.taxid.values]['tree'].values\n",
        "    mini['level'] = sild.loc[mini.taxid.values]['level'].values\n",
        "    mini['phylum'] = [x.split(';')[0] for x in mini.tree]\n",
        "    print(mini.phylum.unique())\n",
        "    # if genus\n",
        "    if ranks == 'genus':\n",
        "        mini = mini[mini.level=='genus'] \n",
        "        mini['ranks'] = namesdmp.loc[mini.taxid][2].values \n",
        "        mini.index = mini.read  \n",
        "        for i in mini.index[mini.duplicated(subset='read', keep=False)].unique():\n",
        "            minil = list(mini.loc[i].taxid.values)\n",
        "            if minil.count(minil[0]) != len(minil):\n",
        "                mini.drop(i)\n",
        "        mini.drop_duplicates(subset='read', keep='first', inplace=True)\n",
        "    # if family\n",
        "    if ranks == 'family':\n",
        "        mini = mini[(mini.level=='genus')|(mini.level=='family')] # 9349\n",
        "        mini['ranks'] = namesdmp.loc[mini.taxid][2].values   \n",
        "        mini.index = mini.read\n",
        "        for i in mini.index[mini.duplicated(subset='read', keep=False)].unique():\n",
        "            minil = list(mini.loc[i].taxid.values)\n",
        "            if minil.count(minil[0]) != len(minil):\n",
        "                minil2 = [x.split(';')[-2] for x in mini.loc[i].tree.values]\n",
        "                if minil2.count(minil2[0]) == len(minil2):\n",
        "                    mini['ranks'].loc[i] = minil2[0]\n",
        "                else: \n",
        "                    mini.drop(i)\n",
        "        mini.drop_duplicates(subset='read', keep='first', inplace=True)           \n",
        "        mini['genuss'] = [sild.loc[x].tree.split(';')[-2] for x in mini.taxid]\n",
        "        mini['ranks'][mini.level=='genus'] = mini['genuss'][mini.level=='genus']\n",
        "    mini2 = pd.DataFrame(mini.ranks.value_counts())\n",
        "    if nr==0:\n",
        "        minif = mini2.copy(deep=True)\n",
        "        minif.columns.values[nr] = filename.split('.')[0]\n",
        "    else:\n",
        "        minif = minif.merge(mini2, left_index=True, right_index=True, how='outer')\n",
        "        minif.columns.values[nr] = filename.split('.')[0]\n",
        "    nr = nr+1\n",
        "    \n",
        "# describe all missing bacteria as absent\n",
        "minif = minif.fillna(0) \n",
        "#Above causes warning cant figure out best way to unchain\n",
        "# save the final txt files  \n",
        "minif.to_csv('/content/drive/MyDrive/16SColab/abundanceTables-indiv/minimap2_k12_%s.txt' %ranks, sep='\\t')"
      ],
      "metadata": {
        "id": "AKhCbk6n9x4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate reads per order "
      ],
      "metadata": {
        "id": "yaV5lDAu9cxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "import os\n",
        "import requests\n",
        "import numpy as np\n",
        "# load all files from the SILVA database\n",
        "minit = pd.read_csv('/content/drive/MyDrive/16SColab/SILVA/seqid2taxid.map', sep='\\t', index_col=0, header=None)\n",
        "sild = pd.read_csv('/content/drive/MyDrive/16SColab/SILVA/tax_slv_ssu_138.1.txt', sep='\\t', header=None)\n",
        "sild.columns = ['tree','taxid','level','3','4']\n",
        "sild['ranks'] = [x.split(';')[-2] for x in sild.tree.values]\n",
        "sild['tree'] = [x[0:-1] for x in sild.tree]\n",
        "sild.index = sild.taxid\n",
        "namesdmp = pd.read_csv('/content/drive/MyDrive/16SColab/SILVA/names.dmp', sep='\\t', index_col=0, header=None)\n",
        "# choose rank and month\n",
        "ranks = 'family'\n",
        "# choose dir of sam files\n",
        "dirc = '/content/drive/MyDrive/16SColab/samoutput-indiv/' \n",
        "# create \n",
        "nr = 0\n",
        "for filename in os.listdir(dirc):\n",
        "    print(filename)\n",
        "    \n",
        "    try:\n",
        "        silva_10k = pd.read_csv('/content/drive/MyDrive/16SColab/samoutput-indiv/%s' %filename, sep='\\t', header=None, usecols = [0,2,4,13])\n",
        "    except: \n",
        "        continue\n",
        "    \n",
        "    silva_10k.columns = ['Read_ID', 'id','MS', 'ASs']\n",
        "    silva_10k['ASs'] = silva_10k['ASs'].astype('str')\n",
        "    silva_10k['AS'] = [x.split(':i:')[-1] for x in silva_10k['ASs'].values]\n",
        "    silva_10k.dropna(axis=0, subset=['AS'], inplace=True)\n",
        "    silva_10k['AS'] = silva_10k['AS'].astype('float')\n",
        "    mini = silva_10k[silva_10k['AS'] == silva_10k.groupby('Read_ID')['AS'].transform('max')]\n",
        "    mini = mini[['Read_ID', 'MS', 'AS','id']]              \n",
        "    mini.columns = ['read','score','as','id']\n",
        "    mini = mini[~mini.id.isnull()]\n",
        "    mini['taxid'] = minit.loc[mini.id.values].values\n",
        "    mini['tree'] = sild.loc[mini.taxid.values]['tree'].values\n",
        "    mini['level'] = sild.loc[mini.taxid.values]['level'].values\n",
        "    mini['phylum'] = [x.split(';')[0] for x in mini.tree]\n",
        "    print(mini.phylum.unique())\n",
        "    # if genus\n",
        "    if ranks == 'genus':\n",
        "        mini = mini[mini.level=='genus'] \n",
        "        mini['ranks'] = namesdmp.loc[mini.taxid][2].values \n",
        "        mini.index = mini.read  \n",
        "        for i in mini.index[mini.duplicated(subset='read', keep=False)].unique():\n",
        "            minil = list(mini.loc[i].taxid.values)\n",
        "            if minil.count(minil[0]) != len(minil):\n",
        "                mini.drop(i)\n",
        "        mini.drop_duplicates(subset='read', keep='first', inplace=True)\n",
        "    # if family\n",
        "    if ranks == 'family':\n",
        "        mini = mini[(mini.level=='genus')|(mini.level=='family')] # 9349\n",
        "        mini['ranks'] = namesdmp.loc[mini.taxid][2].values   \n",
        "        mini.index = mini.read\n",
        "        for i in mini.index[mini.duplicated(subset='read', keep=False)].unique():\n",
        "            minil = list(mini.loc[i].taxid.values)\n",
        "            minil2 = [x.split(';')[-3] for x in mini.loc[i].tree.values]\n",
        "            mini['ranks'].loc[i] = minil2[0]\n",
        "        mini.drop_duplicates(subset='read', keep='first', inplace=True)           \n",
        "        mini['familys'] = [sild.loc[x].tree.split(';')[-3] for x in mini.taxid]\n",
        "        mini['ranks'][mini.level=='family'] = mini['familys'][mini.level=='family']\n",
        "    mini2 = pd.DataFrame(mini.familys.value_counts())\n",
        "    if nr==0:\n",
        "        minif = mini2.copy(deep=True)\n",
        "        minif.columns.values[nr] = filename.split('.')[0]\n",
        "    else:\n",
        "        minif = minif.merge(mini2, left_index=True, right_index=True, how='outer')\n",
        "        minif.columns.values[nr] = filename.split('.')[0]\n",
        "        minif = minif.copy(deep=True)\n",
        "    nr = nr+1\n",
        "    \n",
        "# describe all missing bacteria as absent\n",
        "minif = minif.fillna(0) \n",
        "#Above causes warning cant figure out best way to unchain. \n",
        "# save the final txt files  \n",
        "minif.to_csv('/content/drive/MyDrive/16SColab/abundanceTables-indiv/minimap2_k12_order.txt', sep='\\t')"
      ],
      "metadata": {
        "id": "yvzqLh-3Qv5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A python script to output taxanomic ID as well as reads assigned per taxon. This is useful for later steps in annotation of figures. If you could like to challenge your understanding of the code you can try to adapt the script to output the taxanomic id for higher taxa levels (currently set to Genus). "
      ],
      "metadata": {
        "id": "n3hiB14nulEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "import os\n",
        "import requests\n",
        "import numpy as np\n",
        "# load all files from the SILVA database\n",
        "minit = pd.read_csv('/content/drive/MyDrive/16SColab/SILVA/seqid2taxid.map', sep='\\t', index_col=0, header=None)\n",
        "sild = pd.read_csv('/content/drive/MyDrive/16SColab/SILVA/tax_slv_ssu_138.1.txt', sep='\\t', header=None)\n",
        "sild.columns = ['tree','taxid','level','3','4']\n",
        "sild['ranks'] = [x.split(';')[-2] for x in sild.tree.values]\n",
        "sild['tree'] = [x[0:-1] for x in sild.tree]\n",
        "sild.index = sild.taxid\n",
        "namesdmp = pd.read_csv('/content/drive/MyDrive/16SColab/SILVA/names.dmp', sep='\\t', index_col=0, header=None)\n",
        "# choose rank and month\n",
        "ranks = 'genus'\n",
        "# choose dir of sam files\n",
        "dirc = '/content/drive/MyDrive/16SColab/samoutput-indiv/' \n",
        "# create \n",
        "nr = 0\n",
        "for filename in os.listdir(dirc):\n",
        "    print(filename)\n",
        "    \n",
        "    try:\n",
        "        silva_10k = pd.read_csv('/content/drive/MyDrive/16SColab/samoutput-indiv/%s' %filename, \n",
        "                         sep='\\t', header=None, usecols = [0,2,4,13])\n",
        "    except: \n",
        "        continue\n",
        "    \n",
        "    silva_10k.columns = ['Read_ID', 'id','MS', 'ASs']\n",
        "    silva_10k['ASs'] = silva_10k['ASs'].astype('str')\n",
        "    silva_10k['AS'] = [x.split(':i:')[-1] for x in silva_10k['ASs'].values]\n",
        "    silva_10k.dropna(axis=0, subset=['AS'], inplace=True)\n",
        "    silva_10k['AS'] = silva_10k['AS'].astype('float')\n",
        "    mini = silva_10k[silva_10k['AS'] == silva_10k.groupby('Read_ID')['AS'].transform('max')]\n",
        "    mini = mini[['Read_ID', 'MS', 'AS','id']]              \n",
        "    mini.columns = ['read','score','as','id']\n",
        "    mini = mini[~mini.id.isnull()]\n",
        "    mini['taxid'] = minit.loc[mini.id.values].values\n",
        "    mini['tree'] = sild.loc[mini.taxid.values]['tree'].values\n",
        "    mini['level'] = sild.loc[mini.taxid.values]['level'].values\n",
        "    mini['phylum'] = [x.split(';')[0] for x in mini.tree]\n",
        "    #print(mini.phylum.unique())\n",
        "    # if genus\n",
        "    if ranks == 'genus':\n",
        "        mini = mini[mini.level=='genus'] \n",
        "        mini['ranks'] = namesdmp.loc[mini.taxid][2].values \n",
        "        mini.index = mini.read  \n",
        "        for i in mini.index[mini.duplicated(subset='read', keep=False)].unique():\n",
        "            minil = list(mini.loc[i].taxid.values)\n",
        "            if minil.count(minil[0]) != len(minil):\n",
        "                mini.drop(i)\n",
        "        mini.drop_duplicates(subset='read', keep='first', inplace=True)\n",
        "    # if family\n",
        "    #print(mini)\n",
        "    rank_taxid = mini[[\"ranks\", \"taxid\"]]\n",
        "    mini2 = pd.DataFrame(mini.ranks.value_counts())\n",
        "\n",
        "    if nr==0:\n",
        "        #print('4')\n",
        "        minif = mini2.copy(deep=True)\n",
        "        minif.columns.values[nr] = filename.split('.')[0]\n",
        "        mini3 = mini.ranks.value_counts().rename_axis('ranks').to_frame(filename.split('.')[0])\n",
        "        mini4 = pd.merge(mini3, rank_taxid, on=\"ranks\", how=\"left\")\n",
        "        mini5 = mini4.drop_duplicates()\n",
        "        minix= mini5.copy(deep=True)\n",
        "    else:\n",
        "        minif = minif.merge(mini2, left_index=True, right_index=True, how='outer')\n",
        "        minif.columns.values[nr] = filename.split('.')[0]\n",
        "        mini3 = mini.ranks.value_counts().rename_axis('ranks').to_frame(filename.split('.')[0])\n",
        "\n",
        "        merged = pd.merge(minix, mini3, how='inner', left_on=['ranks'], right_on=['ranks'])\n",
        "        minix= merged\n",
        "        \"\"\"\n",
        "        miniz = pd.merge(minix, mini3, on=\"ranks\")\n",
        "        print(miniz)\n",
        "        minix.columns.values[nr] = filename.split('.')[0]\n",
        "        miniq = miniz.loc[:, ['ranks', 'taxid', 'mock3','mock5',]]\n",
        "        miniq2=miniq.rename(columns = {'reads_y':'mock3','reads_x':'mock5'})\n",
        "\n",
        "        print(miniq)\n",
        "        \"\"\"\n",
        "        \n",
        "    nr = nr+1\n",
        "    \n",
        "# describe all missing bacteria as absent\n",
        "minif = minif.fillna(0) \n",
        "minix = minix.fillna(0) \n",
        "#Above causes warning cant figure out best way to unchain\n",
        "# save the final txt files  \n",
        "df = pd.DataFrame(merged)\n",
        "temp_cols=df.columns.tolist()\n",
        "index=df.columns.get_loc(\"taxid\")\n",
        "new_cols=temp_cols[index:index+1] + temp_cols[0:index] + temp_cols[index+1:]\n",
        "df=df[new_cols]\n",
        "minif.to_csv('minimap2_k12_%s.txt' %ranks, sep='\\t')\n",
        "df.to_csv('/content/drive/MyDrive/16SColab/abundanceTables-indiv/minimap2_k12_%s_taxids.txt' %ranks, sep='\\t')\n"
      ],
      "metadata": {
        "id": "m_g0D_gduhvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# End of python based session\n",
        "\n",
        "Now we have our abundance tables we are going to look into comparisons of diversity and abundance of taxa across sites. \n",
        "\n",
        "\n",
        "For more examples of bioinformatic pipelines in colab check out this tutorial from another [lab](https://colab.research.google.com/drive/1BK2-d7xcF1puNO4h1lFgRMhgnKAuTKzh?usp=sharing)."
      ],
      "metadata": {
        "id": "tXkbzGu2wPD7"
      }
    }
  ]
}